#!/usr/bin/env python3
"""
X (Twitter) æ–‡ç« ä¸‹è½½å·¥å…·

å°† X å¹³å°çš„æ¨æ–‡/æ–‡ç« é“¾æ¥ä¸‹è½½ä¸ºæœ¬åœ° Markdown æ–‡ä»¶ï¼Œ
å›¾ç‰‡ç‹¬ç«‹ä¿å­˜åˆ°æ–‡ä»¶å¤¹ä¸­å¹¶åœ¨ Markdown ä¸­å¼•ç”¨ã€‚

Usage:
    python x_downloader.py <tweet_url> [--output-dir <dir>]
    python x_downloader.py "https://x.com/user/status/123456"
"""

import argparse
import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

try:
    import requests
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–: requests")
    print("   è¯·è¿è¡Œ: pip install requests")
    sys.exit(1)


# ============================================================
# 1. URL è§£æ
# ============================================================

# æ”¯æŒçš„ URL åŸŸå
SUPPORTED_DOMAINS = [
    "x.com",
    "twitter.com",
    "mobile.twitter.com",
    "fxtwitter.com",
    "fixupx.com",
    "vxtwitter.com",
    "nitter.net",
]

# åŒ¹é… /user/status/ID æˆ– /i/status/ID
STATUS_PATTERN = re.compile(r"/(?:[\w]+)/status(?:es)?/(\d+)")


def parse_tweet_url(url: str) -> tuple[str, str]:
    """
    ä» X/Twitter URL ä¸­è§£æå‡º screen_name å’Œ tweet_id.

    Returns:
        (screen_name, tweet_id)

    Raises:
        ValueError: å¦‚æœ URL æ ¼å¼æ— æ•ˆ
    """
    url = url.strip()

    # ç¡®ä¿æœ‰ scheme
    if not url.startswith(("http://", "https://")):
        url = "https://" + url

    parsed = urlparse(url)
    domain = parsed.hostname

    if not domain:
        raise ValueError(f"æ— æ³•è§£æ URL: {url}")

    # å»æ‰ www. å‰ç¼€
    domain = domain.lstrip("www.")

    if domain not in SUPPORTED_DOMAINS:
        raise ValueError(
            f"ä¸æ”¯æŒçš„åŸŸå: {domain}\n"
            f"æ”¯æŒçš„åŸŸå: {', '.join(SUPPORTED_DOMAINS)}"
        )

    match = STATUS_PATTERN.search(parsed.path)
    if not match:
        raise ValueError(f"URL ä¸­æœªæ‰¾åˆ°æ¨æ–‡ ID: {url}")

    tweet_id = match.group(1)

    # æå– screen_nameï¼ˆè·¯å¾„ç¬¬ä¸€æ®µï¼‰
    path_parts = parsed.path.strip("/").split("/")
    screen_name = path_parts[0] if path_parts else "unknown"

    return screen_name, tweet_id


# ============================================================
# 2. FxTwitter API æ•°æ®è·å–
# ============================================================

FXTWITTER_API = "https://api.fxtwitter.com"

# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæ­£å¸¸æµè§ˆå™¨
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/json",
}


def fetch_tweet(tweet_id: str, screen_name: str = "i") -> dict:
    """
    é€šè¿‡ FxTwitter API è·å–æ¨æ–‡æ•°æ®ã€‚

    Args:
        tweet_id: æ¨æ–‡ ID
        screen_name: ç”¨æˆ·åï¼ˆç”¨ 'i' ä¹Ÿå¯ä»¥ï¼‰

    Returns:
        æ¨æ–‡æ•°æ®å­—å…¸
    """
    url = f"{FXTWITTER_API}/{screen_name}/status/{tweet_id}"
    print(f"ğŸ“¡ æ­£åœ¨è·å–æ¨æ–‡æ•°æ®: {url}")

    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
    except requests.exceptions.ConnectionError:
        raise ConnectionError(
            "âš ï¸  æ— æ³•è¿æ¥åˆ° FxTwitter APIã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨ä»£ç†ã€‚"
        )
    except requests.exceptions.HTTPError as e:
        if resp.status_code == 404:
            raise ValueError(f"æ¨æ–‡ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤ (ID: {tweet_id})")
        raise ConnectionError(f"API è¯·æ±‚å¤±è´¥: {e}")

    data = resp.json()

    if data.get("code") != 200:
        raise ValueError(
            f"API è¿”å›é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"
        )

    return data.get("tweet", {})


# ============================================================
# 3. å›¾ç‰‡ä¸‹è½½
# ============================================================


def download_image(url: str, save_path: Path) -> bool:
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡åˆ°æŒ‡å®šè·¯å¾„ã€‚

    Returns:
        True å¦‚æœä¸‹è½½æˆåŠŸ
    """
    try:
        resp = requests.get(url, headers=HEADERS, timeout=60, stream=True)
        resp.raise_for_status()

        save_path.parent.mkdir(parents=True, exist_ok=True)

        with open(save_path, "wb") as f:
            for chunk in resp.iter_content(chunk_size=8192):
                f.write(chunk)

        size_kb = save_path.stat().st_size / 1024
        print(f"   âœ… å·²ä¸‹è½½: {save_path.name} ({size_kb:.1f} KB)")
        return True

    except Exception as e:
        print(f"   âŒ ä¸‹è½½å¤±è´¥ {save_path.name}: {e}")
        return False


def download_tweet_images(
    tweet_data: dict, images_dir: Path
) -> list[dict]:
    """
    ä¸‹è½½æ¨æ–‡ä¸­çš„æ‰€æœ‰å›¾ç‰‡ã€‚

    Returns:
        ä¸‹è½½ç»“æœåˆ—è¡¨: [{"url": ..., "local_path": ..., "filename": ...}, ...]
    """
    media_list = tweet_data.get("media", {})
    if not media_list:
        # å°è¯•å¦ä¸€ç§æ ¼å¼
        media_list = tweet_data.get("medias", [])

    photos = []

    # media å¯èƒ½æ˜¯ dict å« photos åˆ—è¡¨ï¼Œä¹Ÿå¯èƒ½æ˜¯ list
    if isinstance(media_list, dict):
        photos = media_list.get("photos", [])
        # å¦‚æœ all å­—æ®µä¸‹æœ‰æ›´å¤šä¿¡æ¯
        all_media = media_list.get("all", [])
        if not photos and all_media:
            photos = [m for m in all_media if m.get("type") == "photo"]
    elif isinstance(media_list, list):
        photos = [m for m in media_list if m.get("type") == "photo"]

    if not photos:
        print("ğŸ“· è¯¥æ¨æ–‡æ²¡æœ‰å›¾ç‰‡")
        return []

    print(f"ğŸ“· å‘ç° {len(photos)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")

    results = []
    for i, photo in enumerate(photos, 1):
        img_url = photo.get("url", "")
        if not img_url:
            continue

        # ä» URL æå–æ–‡ä»¶æ‰©å±•å
        ext = _get_image_extension(img_url)
        filename = f"{i}{ext}"
        save_path = images_dir / filename

        if download_image(img_url, save_path):
            results.append({
                "url": img_url,
                "local_path": str(save_path),
                "filename": filename,
                "alt": photo.get("altText", f"å›¾ç‰‡{i}"),
            })

    return results


def _get_image_extension(url: str) -> str:
    """ä» URL ä¸­æå–å›¾ç‰‡æ‰©å±•å"""
    # Twitter å›¾ç‰‡ URL æ ¼å¼:
    # https://pbs.twimg.com/media/xxx.jpg
    # https://pbs.twimg.com/media/xxx?format=jpg&name=large
    parsed = urlparse(url)
    path = parsed.path

    # ç›´æ¥ä»è·¯å¾„è·å–æ‰©å±•å
    _, ext = os.path.splitext(path)
    if ext in (".jpg", ".jpeg", ".png", ".gif", ".webp"):
        return ext

    # ä»æŸ¥è¯¢å‚æ•°è·å–
    if "format=" in url:
        match = re.search(r"format=(\w+)", url)
        if match:
            return f".{match.group(1)}"

    return ".jpg"  # é»˜è®¤


# ============================================================
# 4. X Article (é•¿æ–‡ç« ) è§£æ
# ============================================================


def convert_article_to_markdown(
    article: dict,
    entity_map: list,
    images_dir: Path,
    images_dir_name: str,
    media_url_map: dict | None = None,
) -> tuple[str, int]:
    """
    å°† X Article çš„ Draft.js blocks è½¬æ¢ä¸º Markdownã€‚

    Returns:
        (markdown_text, downloaded_image_count)
    """
    content = article.get("content", {})
    blocks = content.get("blocks", [])

    # æ„å»º entity map ç´¢å¼• (key -> value)
    entity_dict = {}
    if isinstance(entity_map, list):
        for item in entity_map:
            entity_dict[str(item.get("key", ""))] = item.get("value", {})
    elif isinstance(entity_map, dict):
        entity_dict = entity_map

    lines = []
    img_counter = 0
    list_counter = 0
    prev_type = ""

    for block in blocks:
        block_type = block.get("type", "unstyled")
        text = block.get("text", "")
        entity_ranges = block.get("entityRanges", [])
        inline_styles = block.get("inlineStyleRanges", [])

        # åº”ç”¨å†…è”æ ·å¼ (Bold, Italic)
        styled_text = _apply_inline_styles(text, inline_styles)

        # åº”ç”¨ entityï¼ˆé“¾æ¥ç­‰ï¼‰
        styled_text = _apply_entities(styled_text, text, entity_ranges, entity_dict)

        # åˆ—è¡¨è®¡æ•°å™¨ç®¡ç†
        if block_type != "ordered-list-item":
            list_counter = 0

        # å—ç±»å‹åˆ†éš”
        if prev_type in ("ordered-list-item", "unordered-list-item") and \
           block_type not in ("ordered-list-item", "unordered-list-item"):
            lines.append("")  # åˆ—è¡¨ç»“æŸåç©ºè¡Œ

        if block_type == "header-one":
            lines.append(f"# {styled_text}")
            lines.append("")
        elif block_type == "header-two":
            lines.append(f"## {styled_text}")
            lines.append("")
        elif block_type == "header-three":
            lines.append(f"### {styled_text}")
            lines.append("")
        elif block_type == "ordered-list-item":
            list_counter += 1
            lines.append(f"{list_counter}. {styled_text}")
        elif block_type == "unordered-list-item":
            lines.append(f"- {styled_text}")
        elif block_type == "blockquote":
            lines.append(f"> {styled_text}")
            lines.append("")
        elif block_type == "atomic":
            # åŸå­å—ï¼šä»£ç å—ã€å›¾ç‰‡ç­‰
            result = _render_atomic_block(entity_ranges, entity_dict,
                                          images_dir, images_dir_name,
                                          img_counter, media_url_map)
            if result:
                md_text, new_imgs = result
                lines.append(md_text)
                lines.append("")
                img_counter += new_imgs
        elif block_type == "unstyled":
            if styled_text.strip():
                lines.append(styled_text)
                lines.append("")
            else:
                lines.append("")
        else:
            # æœªçŸ¥ç±»å‹ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬
            if styled_text.strip():
                lines.append(styled_text)
                lines.append("")

        prev_type = block_type

    return "\n".join(lines), img_counter


def _apply_inline_styles(text: str, styles: list) -> str:
    """åº”ç”¨å†…è”æ ·å¼ï¼ˆBold, Italicï¼‰åˆ°æ–‡æœ¬"""
    if not styles or not text:
        return text

    # æŒ‰ offset é™åºæ’åˆ—ï¼Œä»åå‘å‰æ›¿æ¢é¿å…åç§»é”™è¯¯
    sorted_styles = sorted(styles, key=lambda s: s["offset"], reverse=True)

    chars = list(text)
    for style in sorted_styles:
        offset = style.get("offset", 0)
        length = style.get("length", 0)
        style_type = style.get("style", "")

        if offset + length > len(chars):
            continue

        segment = "".join(chars[offset:offset + length])

        if style_type == "Bold":
            replacement = f"**{segment}**"
        elif style_type == "Italic":
            replacement = f"*{segment}*"
        else:
            replacement = segment

        chars[offset:offset + length] = list(replacement)

    return "".join(chars)


def _apply_entities(styled_text: str, original_text: str,
                    entity_ranges: list, entity_dict: dict) -> str:
    """åº”ç”¨å®ä½“ï¼ˆé“¾æ¥ç­‰ï¼‰åˆ°æ–‡æœ¬"""
    if not entity_ranges:
        return styled_text

    for er in sorted(entity_ranges, key=lambda e: e.get("offset", 0), reverse=True):
        key = str(er.get("key", ""))
        entity = entity_dict.get(key, {})
        entity_type = entity.get("type", "")
        entity_data = entity.get("data", {})

        if entity_type == "LINK":
            url = entity_data.get("url", "")
            offset = er.get("offset", 0)
            length = er.get("length", 0)
            link_text = original_text[offset:offset + length]
            if link_text and url:
                styled_text = styled_text.replace(
                    link_text, f"[{link_text}]({url})", 1
                )
        # TWEMOJI ä¸å¤„ç†ï¼Œä¿ç•™åŸæ–‡ emoji

    return styled_text


def _render_atomic_block(entity_ranges: list, entity_dict: dict,
                         images_dir: Path, images_dir_name: str,
                         img_counter: int,
                         media_url_map: dict | None = None,
                         ) -> tuple[str, int] | None:
    """æ¸²æŸ“åŸå­å—ï¼ˆä»£ç å—ã€å›¾ç‰‡ç­‰ï¼‰"""
    if not entity_ranges:
        return None

    er = entity_ranges[0]
    key = str(er.get("key", ""))
    entity = entity_dict.get(key, {})
    entity_type = entity.get("type", "")
    entity_data = entity.get("data", {})

    if entity_type == "MARKDOWN":
        # ä»£ç å—
        md_content = entity_data.get("markdown", "")
        return md_content.strip(), 0

    elif entity_type == "IMAGE":
        # æ–‡ç« å†…åµŒå›¾ç‰‡ï¼ˆç›´æ¥å« URLï¼‰
        img_url = entity_data.get("src", "") or entity_data.get("url", "")
        if img_url:
            img_counter += 1
            ext = _get_image_extension(img_url)
            filename = f"article_{img_counter}{ext}"
            save_path = images_dir / filename
            if download_image(img_url, save_path):
                alt = entity_data.get("alt", f"æ–‡ç« å›¾ç‰‡{img_counter}")
                return f"![{alt}]({images_dir_name}/{filename})", 1
        return None

    elif entity_type == "MEDIA":
        # åª’ä½“å†…å®¹ï¼šé€šè¿‡ mediaItems -> mediaId æŸ¥æ‰¾ URL
        img_url = ""
        caption = entity_data.get("caption", "")

        # æ–¹å¼1ï¼šé€šè¿‡ media_url_map æŸ¥æ‰¾ï¼ˆmediaItems -> mediaIdï¼‰
        media_items = entity_data.get("mediaItems", [])
        if media_items and media_url_map:
            media_id = media_items[0].get("mediaId", "")
            img_url = media_url_map.get(str(media_id), "")

        # æ–¹å¼2ï¼šç›´æ¥ä» media_info è·å–
        if not img_url:
            media_info = entity_data.get("media_info", {})
            img_url = media_info.get("original_img_url", "")

        if img_url:
            img_counter += 1
            ext = _get_image_extension(img_url)
            filename = f"article_{img_counter}{ext}"
            save_path = images_dir / filename
            if download_image(img_url, save_path):
                alt = caption.strip() if caption else f"æ–‡ç« å›¾ç‰‡{img_counter}"
                return f"![{alt}]({images_dir_name}/{filename})", 1
        return None

    return None


def generate_article_markdown(
    tweet_data: dict,
    article: dict,
    images_dir: Path,
    images_dir_name: str,
    original_url: str,
) -> tuple[str, int]:
    """
    å°† X Article è½¬ä¸ºå®Œæ•´ Markdownã€‚

    Returns:
        (markdown_text, total_image_count)
    """
    author = tweet_data.get("author", {})
    author_name = author.get("name", "Unknown")
    author_screen = author.get("screen_name", "unknown")

    title = article.get("title", "")
    created_at = article.get("created_at", "")
    if created_at:
        try:
            dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
            created_at = dt.strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, TypeError):
            pass

    likes = format_number(tweet_data.get("likes", 0))
    retweets = format_number(tweet_data.get("retweets", 0))
    replies = format_number(tweet_data.get("replies", 0))
    views = format_number(tweet_data.get("views", 0))

    lines = []

    # æ ‡é¢˜
    if title:
        lines.append(f"# {title}")
    else:
        lines.append(f"# @{author_screen} çš„æ–‡ç« ")
    lines.append("")

    # ä½œè€…å’Œå…ƒä¿¡æ¯
    lines.append(f"> âœï¸ @{author_screen} ({author_name})")
    lines.append(f"> ğŸ“… {created_at} | "
                 f"â¤ï¸ {likes} | "
                 f"ğŸ” {retweets} | "
                 f"ğŸ’¬ {replies} | "
                 f"ğŸ‘ï¸ {views}")
    lines.append("")

    # å°é¢å›¾
    cover = article.get("cover_media", {})
    cover_img_count = 0
    if cover:
        media_info = cover.get("media_info", {})
        cover_url = media_info.get("original_img_url", "")
        if cover_url:
            ext = _get_image_extension(cover_url)
            filename = f"cover{ext}"
            save_path = images_dir / filename
            print(f"ğŸ“· ä¸‹è½½å°é¢å›¾ç‰‡...")
            if download_image(cover_url, save_path):
                lines.append(f"![å°é¢]({images_dir_name}/{filename})")
                lines.append("")
                cover_img_count = 1

    lines.append("---")
    lines.append("")

    # æ„å»º media_id -> URL æŸ¥æ‰¾è¡¨
    media_url_map = {}
    for me in article.get("media_entities", []):
        mid = str(me.get("media_id", ""))
        info = me.get("media_info", {})
        url = info.get("original_img_url", "")
        if mid and url:
            media_url_map[mid] = url

    if media_url_map:
        print(f"ğŸ“· å‘ç° {len(media_url_map)} å¼ æ–‡ç« å†…åµŒå›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")

    # æ–‡ç« æ­£æ–‡
    content = article.get("content", {})
    entity_map = content.get("entityMap", [])

    article_md, article_img_count = convert_article_to_markdown(
        article, entity_map, images_dir, images_dir_name, media_url_map
    )
    lines.append(article_md)

    # æ¥æº
    lines.append("")
    lines.append("---")
    lines.append("")
    tweet_url = tweet_data.get("url", original_url)
    lines.append(f"*æ¥æº: [{tweet_url}]({tweet_url})*")
    lines.append("")

    total_images = cover_img_count + article_img_count
    return "\n".join(lines), total_images


# ============================================================
# 5. Markdown ç”Ÿæˆ (æ™®é€šæ¨æ–‡)
# ============================================================


def format_number(num) -> str:
    """æ ¼å¼åŒ–æ•°å­—: 1200 -> 1.2K"""
    if num is None:
        return "0"
    if isinstance(num, str):
        try:
            num = int(num)
        except ValueError:
            return num
    if num >= 1_000_000:
        return f"{num / 1_000_000:.1f}M"
    if num >= 1_000:
        return f"{num / 1_000:.1f}K"
    return str(num)


def format_tweet_date(date_str: str) -> str:
    """æ ¼å¼åŒ–æ¨æ–‡æ—¥æœŸ"""
    if not date_str:
        return "æœªçŸ¥æ—¥æœŸ"
    try:
        # FxTwitter è¿”å›çš„æ—¥æœŸæ ¼å¼: "Wed Oct 10 20:19:24 +0000 2018"
        dt = datetime.strptime(date_str, "%a %b %d %H:%M:%S %z %Y")
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError):
        return date_str


def generate_markdown(
    tweet_data: dict,
    images: list[dict],
    images_dir_name: str,
    original_url: str,
) -> str:
    """
    å°†æ¨æ–‡æ•°æ®è½¬ä¸º Markdown æ ¼å¼å­—ç¬¦ä¸²ã€‚
    """
    author = tweet_data.get("author", {})
    author_name = author.get("name", "Unknown")
    author_screen = author.get("screen_name", "unknown")
    author_avatar = author.get("avatar_url", "")

    tweet_text = tweet_data.get("text", "")
    created_at = format_tweet_date(tweet_data.get("created_at", ""))
    tweet_url = tweet_data.get("url", original_url)

    likes = format_number(tweet_data.get("likes", 0))
    retweets = format_number(tweet_data.get("retweets", 0))
    replies = format_number(tweet_data.get("replies", 0))
    views = format_number(tweet_data.get("views", 0))

    lines = []

    # æ ‡é¢˜
    lines.append(f"# @{author_screen} ({author_name}) çš„æ¨æ–‡\n")

    # å…ƒä¿¡æ¯
    lines.append(f"> ğŸ“… {created_at} | "
                 f"â¤ï¸ {likes} | "
                 f"ğŸ” {retweets} | "
                 f"ğŸ’¬ {replies} | "
                 f"ğŸ‘ï¸ {views}\n")

    lines.append("---\n")

    # æ­£æ–‡
    if tweet_text:
        lines.append(f"{tweet_text}\n")

    # å›¾ç‰‡
    if images:
        lines.append("")
        for img in images:
            alt = img.get("alt", "å›¾ç‰‡")
            rel_path = f"{images_dir_name}/{img['filename']}"
            lines.append(f"![{alt}]({rel_path})\n")

    # è§†é¢‘æç¤º
    media = tweet_data.get("media", {})
    videos = []
    if isinstance(media, dict):
        videos = media.get("videos", [])
    if videos:
        lines.append("\n> ğŸ¬ è¯¥æ¨æ–‡åŒ…å«è§†é¢‘ï¼Œè¯·è®¿é—®åŸæ–‡æŸ¥çœ‹\n")

    # å¼•ç”¨æ¨æ–‡
    quote = tweet_data.get("quote")
    if quote:
        lines.append("\n---\n")
        lines.append("### å¼•ç”¨æ¨æ–‡\n")
        quote_author = quote.get("author", {})
        quote_screen = quote_author.get("screen_name", "unknown")
        quote_text = quote.get("text", "")
        lines.append(f"> **@{quote_screen}**: {quote_text}\n")

    # æ¥æº
    lines.append("\n---\n")
    lines.append(f"*æ¥æº: [{tweet_url}]({tweet_url})*\n")

    return "\n".join(lines)


# ============================================================
# 6. Threadï¼ˆæ¨æ–‡ä¸²ï¼‰æ”¯æŒ
# ============================================================


def fetch_thread(tweet_data: dict, screen_name: str) -> list[dict]:
    """
    å°è¯•è·å–æ¨æ–‡æ‰€åœ¨çš„ Thread.

    ç®€å•ç­–ç•¥: å¦‚æœæ¨æ–‡æœ‰ replying_to ä¸”å›å¤è‡ªå·±çš„æ¨æ–‡ï¼Œ
    åˆ™å‘ä¸Šè¿½æº¯è·å–æ•´ä¸ª threadã€‚

    Returns:
        Thread ä¸­æ‰€æœ‰æ¨æ–‡çš„åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å‡åºï¼‰
    """
    thread = [tweet_data]

    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹è¯ / thread ä¿¡æ¯
    # FxTwitter API ä¸­ conversation ç›¸å…³å­—æ®µ
    replying_to = tweet_data.get("replying_to")
    tweet_author = tweet_data.get("author", {}).get("screen_name", "")

    if not replying_to:
        return thread

    # å¦‚æœæ˜¯å›å¤è‡ªå·±çš„æ¨æ–‡ï¼Œå°è¯•å‘ä¸Šè¿½æº¯ (æœ€å¤š20æ¡)
    current = tweet_data
    visited = {tweet_data.get("id")}
    max_depth = 20

    for _ in range(max_depth):
        reply_id = current.get("replying_to_status")
        if not reply_id or reply_id in visited:
            break

        try:
            parent = fetch_tweet(reply_id, screen_name)
            parent_author = parent.get("author", {}).get("screen_name", "")

            # åªè¿½æº¯åŒä¸€ä½œè€…çš„ thread
            if parent_author.lower() != tweet_author.lower():
                break

            visited.add(reply_id)
            thread.insert(0, parent)  # æ’å…¥åˆ°æœ€å‰é¢
            current = parent
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

        except Exception:
            break

    if len(thread) > 1:
        print(f"ğŸ§µ å‘ç° Threadï¼Œå…± {len(thread)} æ¡æ¨æ–‡")

    return thread


# ============================================================
# 7. ä¸»æµç¨‹
# ============================================================


def download_tweet(
    url: str,
    output_dir: str = "output",
    include_thread: bool = True,
) -> str:
    """
    ä¸»å‡½æ•°ï¼šä¸‹è½½æ¨æ–‡å¹¶ä¿å­˜ä¸º Markdownã€‚

    Args:
        url: X/Twitter æ¨æ–‡é“¾æ¥
        output_dir: è¾“å‡ºç›®å½•
        include_thread: æ˜¯å¦å°è¯•è·å–æ•´ä¸ª Thread

    Returns:
        ç”Ÿæˆçš„ Markdown æ–‡ä»¶è·¯å¾„
    """
    print(f"\n{'='*50}")
    print(f"ğŸ”— X æ–‡ç« ä¸‹è½½å·¥å…·")
    print(f"{'='*50}\n")

    # 1. è§£æ URL
    screen_name, tweet_id = parse_tweet_url(url)
    print(f"ğŸ‘¤ ç”¨æˆ·: @{screen_name}")
    print(f"ğŸ†” æ¨æ–‡ ID: {tweet_id}\n")

    # 2. è·å–æ¨æ–‡æ•°æ®
    tweet_data = fetch_tweet(tweet_id, screen_name)

    # å®é™…ä½œè€…å¯èƒ½ä¸ URL ä¸­çš„ä¸åŒ
    actual_author = tweet_data.get("author", {}).get("screen_name", screen_name)

    # 3. æ„å»ºè¾“å‡ºè·¯å¾„
    output_path = Path(output_dir)
    base_name = f"{actual_author}_{tweet_id}"
    md_path = output_path / f"{base_name}.md"
    images_dir = output_path / f"{base_name}_images"
    images_dir_name = f"{base_name}_images"

    output_path.mkdir(parents=True, exist_ok=True)

    # 4. æ£€æŸ¥æ˜¯å¦ä¸º X Article (é•¿æ–‡ç« )
    article = tweet_data.get("article")
    if article and article.get("content"):
        print(f"ğŸ“° æ£€æµ‹åˆ° X Article: {article.get('title', 'æ— æ ‡é¢˜')}")
        final_md, total_images = generate_article_markdown(
            tweet_data, article, images_dir, images_dir_name, url
        )

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(final_md)

        print(f"\n{'='*50}")
        print(f"âœ… ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“„ Markdown: {md_path}")
        if total_images > 0:
            print(f"ğŸ“· å›¾ç‰‡ç›®å½•: {images_dir} ({total_images} å¼ )")
        print(f"{'='*50}\n")
        return str(md_path)

    # 5. æ™®é€šæ¨æ–‡ï¼šè·å– Threadï¼ˆå¯é€‰ï¼‰
    if include_thread:
        tweets = fetch_thread(tweet_data, actual_author)
    else:
        tweets = [tweet_data]

    # 6. å¤„ç†æ¯æ¡æ¨æ–‡
    all_md_parts = []
    total_images = 0

    for idx, tweet in enumerate(tweets):
        if len(tweets) > 1:
            print(f"\n--- æ¨æ–‡ {idx + 1}/{len(tweets)} ---")

        # ä¸‹è½½å›¾ç‰‡
        images = download_tweet_images(tweet, images_dir)
        total_images += len(images)

        # ç”Ÿæˆ Markdown
        md_content = generate_markdown(
            tweet, images, images_dir_name, url
        )

        if len(tweets) > 1 and idx > 0:
            # Thread ä¸­çš„åç»­æ¨æ–‡ç”¨åˆ†éš”çº¿è¿æ¥
            all_md_parts.append("\n---\n---\n")

        all_md_parts.append(md_content)

    # 7. åˆå¹¶å¹¶å†™å…¥æ–‡ä»¶
    final_md = "\n".join(all_md_parts)

    # å¦‚æœæ˜¯ Threadï¼Œæ›´æ–°æ ‡é¢˜
    if len(tweets) > 1:
        first_line = final_md.split("\n")[0]
        final_md = final_md.replace(
            first_line,
            f"# @{actual_author} çš„æ¨æ–‡ä¸² (Thread, å…± {len(tweets)} æ¡)",
            1,
        )

    with open(md_path, "w", encoding="utf-8") as f:
        f.write(final_md)

    # 8. å®ŒæˆæŠ¥å‘Š
    print(f"\n{'='*50}")
    print(f"âœ… ä¸‹è½½å®Œæˆ!")
    print(f"ğŸ“„ Markdown: {md_path}")
    if total_images > 0:
        print(f"ğŸ“· å›¾ç‰‡ç›®å½•: {images_dir} ({total_images} å¼ )")
    print(f"{'='*50}\n")

    return str(md_path)


# ============================================================
# 8. CLI å…¥å£
# ============================================================


def main():
    parser = argparse.ArgumentParser(
        description="X (Twitter) æ–‡ç« ä¸‹è½½å·¥å…· - å°†æ¨æ–‡ä¿å­˜ä¸º Markdown",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python x_downloader.py "https://x.com/elonmusk/status/123456"
  python x_downloader.py "https://twitter.com/user/status/789" -o my_tweets
  python x_downloader.py "https://x.com/user/status/123" --no-thread
        """,
    )

    parser.add_argument(
        "url",
        help="X/Twitter æ¨æ–‡é“¾æ¥",
    )
    parser.add_argument(
        "-o", "--output-dir",
        default="output",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: output)",
    )
    parser.add_argument(
        "--no-thread",
        action="store_true",
        help="ä¸è·å– Threadï¼Œåªä¸‹è½½å•æ¡æ¨æ–‡",
    )

    args = parser.parse_args()

    try:
        download_tweet(
            url=args.url,
            output_dir=args.output_dir,
            include_thread=not args.no_thread,
        )
    except ValueError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)
    except ConnectionError as e:
        print(f"\nâŒ ç½‘ç»œé”™è¯¯: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n\nâ¹ å·²å–æ¶ˆ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æ„å¤–é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
